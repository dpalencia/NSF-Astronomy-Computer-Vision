{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proud-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: 10.5281/zenodo.1048301\n",
    "from marsvision.utilities import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regular-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"X:\\hirise-map-proj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "understood-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "class DeepMarsData(Dataset):\n",
    "    # Wrapper to work with deep mars dataset\n",
    "    def __init__(self, root_dir):\n",
    "        \n",
    "        # AlexNet expects images to be normalized this way.\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        # Get image labels\n",
    "        self.labels = {}\n",
    "        with open(os.path.join(root_dir, \"labels-map-proj.txt\")) as f:\n",
    "            for line in f:\n",
    "                items = line.split()\n",
    "                key, value = items[0], items[1]\n",
    "                self.labels[key] = int(value)\n",
    "                \n",
    "        # Get image filenames\n",
    "        self.image_dir = os.path.join(root_dir, \"map-proj\")\n",
    "        image_names = os.listdir(os.path.join(self.image_dir))\n",
    "        # Take set difference \n",
    "        # to ensure that only labelled images are included\n",
    "        self.image_names = list(set(image_names) & set(self.labels))\n",
    "                                  \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a sample as: {'image': image, 'label': label}\n",
    "        # Return an image with the dimensions 3 x W x H\n",
    "        # Because PyTorch models expect these dimensions as inputs.\n",
    "        # Transpose dimensions:\n",
    "        # (W, H, 3) --> (3, W, H)\n",
    "        img_name = self.image_names[idx]\n",
    "        img = Tensor(\n",
    "            cv2.imread(os.path.join(self.image_dir, img_name))\n",
    "        ).transpose(0, 2)\n",
    "        \n",
    "        # Apply normalize\n",
    "        img = self.normalize(img)\n",
    "    \n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"label\": self.labels[self.image_names[idx]]\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "insured-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Splt into train/validation/test sets\n",
    "# Train/Val/Test: 80/5/15\n",
    "dataset = DeepMarsData(dataset_root)\n",
    "\n",
    "# Define dataloader for the whole dataset\n",
    "dataset_size = len(dataset)\n",
    "num_train_samples = int(dataset_size * .8)\n",
    "num_val_samples = int(dataset_size * .05)\n",
    "num_test_samples = dataset_size - num_train_samples - num_val_samples\n",
    "data_sizes = {\n",
    "    \"train\": num_train_samples,\n",
    "    \"val\": num_val_samples,\n",
    "    \"test\": num_test_samples\n",
    "}\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [num_train_samples, \n",
    "     num_val_samples, \n",
    "     num_test_samples]\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(train_dataset, batch_size = 4),\n",
    "    \"val\": torch.utils.data.DataLoader(train_dataset, batch_size = 4),\n",
    "    \"test\": torch.utils.data.DataLoader(train_dataset, batch_size = 4)\n",
    "}\n",
    "\n",
    "\n",
    "# Define a tiny datset to make quick tweaks to the model training code\n",
    "# Use a small subset of the dataset\n",
    "# So we can train it quickly.\n",
    "dataset_smaller, dataset_larger = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    [ 100, dataset_size - 100 ]\n",
    ")\n",
    "\n",
    "dataset_smaller_size = len(dataset_smaller)\n",
    "num_train_samples_smaller = int(dataset_smaller_size * .8)\n",
    "num_val_samples_smaller = int(dataset_smaller_size * .05)\n",
    "num_test_samples_smaller = dataset_smaller_size - num_train_samples_smaller - num_val_samples_smaller\n",
    "train_dataset_smaller, val_dataset_smaller, test_dataset_smaller = torch.utils.data.random_split(\n",
    "    dataset_smaller, \n",
    "    [num_train_samples_smaller, \n",
    "     num_val_samples_smaller, \n",
    "     num_test_samples_smaller]\n",
    ")\n",
    "\n",
    "data_sizes_smaller = {\n",
    "    \"train\": num_train_samples_smaller,\n",
    "    \"val\": num_val_samples_smaller,\n",
    "    \"test\": num_test_samples_smaller\n",
    "}\n",
    "\n",
    "dataloaders_smaller = {\n",
    "        \"train\": torch.utils.data.DataLoader(train_dataset_smaller, batch_size = 4),\n",
    "        \"val\": torch.utils.data.DataLoader(val_dataset_smaller, batch_size = 4),\n",
    "        \"test\": torch.utils.data.DataLoader(test_dataset_smaller, batch_size = 4)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ultimate-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dpale/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Alexnet and print its architecture.\n",
    "# Notice the last layer of the classifier.\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "guilty-queue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(351.5357, grad_fn=<SelectBackward>),\n",
       " tensor([351.5357, 227.1436, 347.5621, 294.0428], grad_fn=<MaxBackward0>),\n",
       " tensor([379, 384, 818, 599]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Playing with the model/ Making predictions\n",
    "sample = next(iter(dataloaders[\"train\"])) \n",
    "output = model(sample[\"image\"]) # Output tensor of shape: (samples, # of classes)\n",
    "values, indices = torch.max(output, 1) # Get max values of confidence scores output by AlexNet.\n",
    "# Indices = classes\n",
    "# Use torch.max to return class label with highest confidence score.\n",
    "output[0][indices[0]], values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "directed-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "# To change the number of output features, modify the classifier like so.\n",
    "# Classes correspond to indices.\n",
    "num_classes = 7\n",
    "model.classifier[6] = nn.Linear(4096,num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "closing-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to train the model\n",
    "def train_model(dataloaders, data_sizes, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            # Swap training/eval modes depending on phase\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                # Eval switches model's behavior\n",
    "                # Enable eval when we need to evaluate the model.\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate and train.\n",
    "            for sample in dataloaders[phase]:\n",
    "                inputs = Tensor(sample[\"image\"]).to(device)\n",
    "                labels = sample[\"label\"]\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass if in train phase\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                \n",
    "                print(\"Running loss: {} | Running corrects: {}\".format(\n",
    "                    running_loss, running_corrects))\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "                    \n",
    "            epoch_loss = running_loss / data_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / data_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} | Images trained on: {}'.format(\n",
    "                phase, epoch_loss, epoch_acc, data_sizes[phase]))\n",
    "            \n",
    "            # In the eval phase, get the accuracy for this epoch\n",
    "            # If the mode's current state is better than the best model seen so far,\n",
    "            # replace the best model weights\n",
    "            # with the previous best model weights on previous epochs\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "third-toronto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "----------\n",
      "Running loss: 17.23426628112793 | Running corrects: 1\n",
      "Running loss: 31.06283187866211 | Running corrects: 3\n",
      "Running loss: 91.17485809326172 | Running corrects: 4\n",
      "Running loss: 91.17645340878516 | Running corrects: 8\n",
      "Running loss: 108.63595658261329 | Running corrects: 10\n",
      "Running loss: 127.81007271725684 | Running corrects: 11\n",
      "Running loss: 169.22201424557716 | Running corrects: 12\n",
      "Running loss: 185.28279190976173 | Running corrects: 14\n",
      "Running loss: 186.3838058086112 | Running corrects: 17\n",
      "Running loss: 195.52520805317909 | Running corrects: 18\n",
      "Running loss: 223.04490142781287 | Running corrects: 19\n",
      "Running loss: 226.50111132580787 | Running corrects: 22\n",
      "Running loss: 228.63199978787452 | Running corrects: 26\n",
      "Running loss: 276.66123944241554 | Running corrects: 26\n",
      "Running loss: 340.7149656387046 | Running corrects: 27\n",
      "Running loss: 406.1205518813804 | Running corrects: 28\n",
      "Running loss: 441.46519070584327 | Running corrects: 28\n",
      "Running loss: 567.6744955154136 | Running corrects: 29\n",
      "Running loss: 585.8948934646323 | Running corrects: 29\n",
      "Running loss: 659.9873159499839 | Running corrects: 32\n",
      "train Loss: 8.2498 Acc: 0.4000 | Images trained on: 80\n",
      "Running loss: 74.61197662353516 | Running corrects: 2\n",
      "Running loss: 75.06014946103096 | Running corrects: 3\n",
      "val Loss: 15.0120 Acc: 0.6000 | Images trained on: 5\n",
      "Epoch 1/3\n",
      "----------\n",
      "Running loss: 5.812308311462402 | Running corrects: 2\n",
      "Running loss: 13.847650527954102 | Running corrects: 4\n",
      "Running loss: 60.35772895812988 | Running corrects: 5\n",
      "Running loss: 60.358102137368405 | Running corrects: 9\n",
      "Running loss: 72.15180807761499 | Running corrects: 11\n",
      "Running loss: 95.11625509909936 | Running corrects: 14\n",
      "Running loss: 125.98237257651635 | Running corrects: 16\n",
      "Running loss: 150.0596278255398 | Running corrects: 17\n",
      "Running loss: 163.31675177268335 | Running corrects: 20\n",
      "Running loss: 179.94331389121362 | Running corrects: 21\n",
      "Running loss: 206.8499529903347 | Running corrects: 23\n",
      "Running loss: 210.21090870551416 | Running corrects: 26\n",
      "Running loss: 212.40781121901819 | Running corrects: 29\n",
      "Running loss: 260.6181345527584 | Running corrects: 30\n",
      "Running loss: 327.25640206984826 | Running corrects: 30\n",
      "Running loss: 402.611077362817 | Running corrects: 32\n",
      "Running loss: 439.33900170974084 | Running corrects: 32\n",
      "Running loss: 531.4048891609127 | Running corrects: 34\n",
      "Running loss: 556.8676176612789 | Running corrects: 34\n",
      "Running loss: 661.7211409156735 | Running corrects: 36\n",
      "train Loss: 8.2715 Acc: 0.4500 | Images trained on: 80\n",
      "Running loss: 74.61197662353516 | Running corrects: 2\n",
      "Running loss: 75.06014946103096 | Running corrects: 3\n",
      "val Loss: 15.0120 Acc: 0.6000 | Images trained on: 5\n",
      "Epoch 2/3\n",
      "----------\n",
      "Running loss: 2.765836715698242 | Running corrects: 3\n",
      "Running loss: 12.260297775268555 | Running corrects: 5\n",
      "Running loss: 43.33746147155762 | Running corrects: 6\n",
      "Running loss: 43.38750900328159 | Running corrects: 10\n",
      "Running loss: 61.84151138365269 | Running corrects: 12\n",
      "Running loss: 83.44871772825718 | Running corrects: 13\n",
      "Running loss: 108.17631973326206 | Running corrects: 16\n",
      "Running loss: 116.03890909254551 | Running corrects: 18\n",
      "Running loss: 117.29601396620274 | Running corrects: 21\n",
      "Running loss: 129.39667047560215 | Running corrects: 22\n",
      "Running loss: 155.8865882307291 | Running corrects: 23\n",
      "Running loss: 166.40625585615635 | Running corrects: 24\n",
      "Running loss: 168.1463027149439 | Running corrects: 28\n",
      "Running loss: 221.10793448984623 | Running corrects: 30\n",
      "Running loss: 277.8450331836939 | Running corrects: 30\n",
      "Running loss: 339.170259013772 | Running corrects: 31\n",
      "Running loss: 377.2442984730005 | Running corrects: 31\n",
      "Running loss: 490.31539680063725 | Running corrects: 31\n",
      "Running loss: 500.44501926004887 | Running corrects: 33\n",
      "Running loss: 545.7868733555079 | Running corrects: 35\n",
      "train Loss: 6.8223 Acc: 0.4375 | Images trained on: 80\n",
      "Running loss: 74.61197662353516 | Running corrects: 2\n",
      "Running loss: 75.06014946103096 | Running corrects: 3\n",
      "val Loss: 15.0120 Acc: 0.6000 | Images trained on: 5\n",
      "Epoch 3/3\n",
      "----------\n",
      "Running loss: 3.9525530338287354 | Running corrects: 2\n",
      "Running loss: 16.759207010269165 | Running corrects: 2\n",
      "Running loss: 53.583276987075806 | Running corrects: 3\n",
      "Running loss: 56.44048094749451 | Running corrects: 6\n",
      "Running loss: 73.6910150051117 | Running corrects: 8\n",
      "Running loss: 85.48739504814148 | Running corrects: 10\n",
      "Running loss: 127.34840273857117 | Running corrects: 13\n",
      "Running loss: 158.06758761405945 | Running corrects: 14\n",
      "Running loss: 158.6469476222992 | Running corrects: 18\n",
      "Running loss: 185.14273810386658 | Running corrects: 19\n",
      "Running loss: 210.92802786827087 | Running corrects: 21\n",
      "Running loss: 217.2713897228241 | Running corrects: 24\n",
      "Running loss: 221.65014243125916 | Running corrects: 26\n",
      "Running loss: 277.6613805294037 | Running corrects: 26\n",
      "Running loss: 313.68792700767517 | Running corrects: 26\n",
      "Running loss: 367.5136296749115 | Running corrects: 28\n",
      "Running loss: 414.0381314754486 | Running corrects: 28\n",
      "Running loss: 500.96575903892517 | Running corrects: 28\n",
      "Running loss: 519.9728810787201 | Running corrects: 29\n",
      "Running loss: 634.8875081539154 | Running corrects: 31\n",
      "train Loss: 7.9361 Acc: 0.3875 | Images trained on: 80\n",
      "Running loss: 74.61197662353516 | Running corrects: 2\n",
      "Running loss: 75.06014946103096 | Running corrects: 3\n",
      "val Loss: 15.0120 Acc: 0.6000 | Images trained on: 5\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to train AlexNet on the small dataset\n",
    "# Don't expect accuracy to be very good because the dataset used is tiny\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.1)\n",
    "model_transfer = train_model(\n",
    "    dataloaders_smaller, \n",
    "    data_sizes_smaller,\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer_ft, \n",
    "    exp_lr_scheduler,\n",
    "    num_epochs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "latter-extra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.4136,  -0.2587,  -0.5166,   1.9010,  -0.0771,   0.7724,   1.6606],\n",
       "         [ 11.6954,  -2.1430,  -1.1246,   2.8636,   0.2056,  -0.8577,  -7.3382],\n",
       "         [ -5.1375,   7.9908, -30.9122,   2.8507,   7.4008,   0.3204,  34.5531],\n",
       "         [  0.6124,  -0.4629,  -1.1553,  -0.3769,   0.0432,  -0.2078,   0.5029]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 0.9510568380355835)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show an example of crossentropyloss.\n",
    "# Crossentropyloss expects \"raw, unnormalized scores\" for each class.\n",
    "sample = next(iter(dataloaders[\"train\"])) \n",
    "scores = model(sample[\"image\"])\n",
    "scores, criterion(scores, sample[\"label\"]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-component",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
