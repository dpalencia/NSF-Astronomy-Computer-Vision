{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proud-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: 10.5281/zenodo.1048301\n",
    "from marsvision.utilities import DataUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regular-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"X:\\hirise-map-proj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "understood-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "class DeepMarsData(Dataset):\n",
    "    # Wrapper to work with deep mars dataset\n",
    "    def __init__(self, root_dir):\n",
    "        \n",
    "        # AlexNet expects images to be normalized this way.\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        # Get image labels\n",
    "        self.labels = {}\n",
    "        with open(os.path.join(root_dir, \"labels-map-proj.txt\")) as f:\n",
    "            for line in f:\n",
    "                items = line.split()\n",
    "                key, value = items[0], items[1]\n",
    "                self.labels[key] = int(value)\n",
    "                \n",
    "        # Get image filenames\n",
    "        self.image_dir = os.path.join(root_dir, \"map-proj\")\n",
    "        image_names = os.listdir(os.path.join(self.image_dir))\n",
    "        # Take set difference \n",
    "        # to ensure that only labelled images are included\n",
    "        self.image_names = list(set(image_names) & set(self.labels))\n",
    "                                  \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a sample as: {'image': image, 'label': label}\n",
    "        # Return an image with the dimensions 3 x W x H\n",
    "        # Because PyTorch models expect these dimensions as inputs.\n",
    "        # Transpose dimensions:\n",
    "        # (W, H, 3) --> (3, W, H)\n",
    "        img_name = self.image_names[idx]\n",
    "        img = Tensor(\n",
    "            cv2.imread(os.path.join(self.image_dir, img_name))\n",
    "        ).transpose(0, 2)\n",
    "        \n",
    "        # Apply normalize\n",
    "        img = self.normalize(img)\n",
    "    \n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"label\": self.labels[self.image_names[idx]]\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "insured-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Splt into train/validation/test sets\n",
    "# Train/Val/Test: 80/5/15\n",
    "dataset = DeepMarsData(dataset_root)\n",
    "\n",
    "# Define DataUtility for the whole dataset\n",
    "dataset_size = len(dataset)\n",
    "num_train_samples = int(dataset_size * .8)\n",
    "num_val_samples = int(dataset_size * .05)\n",
    "num_test_samples = dataset_size - num_train_samples - num_val_samples\n",
    "data_sizes = {\n",
    "    \"train\": num_train_samples,\n",
    "    \"val\": num_val_samples,\n",
    "    \"test\": num_test_samples\n",
    "}\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [num_train_samples, \n",
    "     num_val_samples, \n",
    "     num_test_samples]\n",
    ")\n",
    "\n",
    "DataLoaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(train_dataset, batch_size = 4),\n",
    "    \"val\": torch.utils.data.DataLoader(train_dataset, batch_size = 4),\n",
    "    \"test\": torch.utils.data.DataLoader(train_dataset, batch_size = 4)\n",
    "}\n",
    "\n",
    "\n",
    "# Define a tiny datset to make quick tweaks to the model training code\n",
    "# Use a small subset of the dataset\n",
    "# So we can train it quickly.\n",
    "dataset_smaller, dataset_larger = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    [ 100, dataset_size - 100 ]\n",
    ")\n",
    "\n",
    "dataset_smaller_size = len(dataset_smaller)\n",
    "num_train_samples_smaller = int(dataset_smaller_size * .8)\n",
    "num_val_samples_smaller = int(dataset_smaller_size * .05)\n",
    "num_test_samples_smaller = dataset_smaller_size - num_train_samples_smaller - num_val_samples_smaller\n",
    "train_dataset_smaller, val_dataset_smaller, test_dataset_smaller = torch.utils.data.random_split(\n",
    "    dataset_smaller, \n",
    "    [num_train_samples_smaller, \n",
    "     num_val_samples_smaller, \n",
    "     num_test_samples_smaller]\n",
    ")\n",
    "\n",
    "data_sizes_smaller = {\n",
    "    \"train\": num_train_samples_smaller,\n",
    "    \"val\": num_val_samples_smaller,\n",
    "    \"test\": num_test_samples_smaller\n",
    "}\n",
    "\n",
    "DataUtilitys_smaller = {\n",
    "        \"train\": torch.utils.data.DataLoader(train_dataset_smaller, batch_size = 4),\n",
    "        \"val\": torch.utils.data.DataLoader(val_dataset_smaller, batch_size = 4),\n",
    "        \"test\": torch.utils.data.DataLoader(test_dataset_smaller, batch_size = 4)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ultimate-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dpale/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Alexnet and print its architecture.\n",
    "# Notice the last layer of the classifier.\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "guilty-queue",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataUtilitys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f2b9a388aab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Playing with the model/ Making predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataUtilitys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Output tensor of shape: (samples, # of classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Get max values of confidence scores output by AlexNet.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Indices = classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataUtilitys' is not defined"
     ]
    }
   ],
   "source": [
    "# Playing with the model/ Making predictions\n",
    "sample = next(iter(DataUtilitys[\"train\"])) \n",
    "output = model(sample[\"image\"]) # Output tensor of shape: (samples, # of classes)\n",
    "values, indices = torch.max(output, 1) # Get max values of confidence scores output by AlexNet.\n",
    "# Indices = classes\n",
    "# Use torch.max to return class label with highest confidence score.\n",
    "output[0][indices[0]], values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "directed-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "# To change the number of output features, modify the classifier like so.\n",
    "# Classes correspond to indices.\n",
    "num_classes = 7\n",
    "model.classifier[6] = nn.Linear(4096,num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "closing-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to train the model\n",
    "def train_model(DataUtilitys, data_sizes, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            # Swap training/eval modes depending on phase\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                # Eval switches model's behavior\n",
    "                # Enable eval when we need to evaluate the model.\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate and train.\n",
    "            for sample in DataUtilitys[phase]:\n",
    "                inputs = Tensor(sample[\"image\"]).to(device)\n",
    "                labels = sample[\"label\"]\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass if in train phase\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                \n",
    "                print(\"Running loss: {} | Running corrects: {}\".format(\n",
    "                    running_loss, running_corrects))\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "                    \n",
    "            epoch_loss = running_loss / data_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / data_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} | Images trained on: {}'.format(\n",
    "                phase, epoch_loss, epoch_acc, data_sizes[phase]))\n",
    "            \n",
    "            # In the eval phase, get the accuracy for this epoch\n",
    "            # If the mode's current state is better than the best model seen so far,\n",
    "            # replace the best model weights\n",
    "            # with the previous best model weights on previous epochs\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "third-toronto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "----------\n",
      "Running loss: 53.625022888183594 | Running corrects: 2\n",
      "Running loss: 763.5785140991211 | Running corrects: 3\n",
      "Running loss: 1040.5077438354492 | Running corrects: 5\n",
      "Running loss: 1149.9172973632812 | Running corrects: 8\n",
      "Running loss: 1505.138671875 | Running corrects: 9\n",
      "Running loss: 1566.3767127990723 | Running corrects: 12\n",
      "Running loss: 1812.4096870422363 | Running corrects: 14\n",
      "Running loss: 1962.161304473877 | Running corrects: 16\n",
      "Running loss: 2214.4402503967285 | Running corrects: 18\n",
      "Running loss: 2297.5878944396973 | Running corrects: 21\n",
      "Running loss: 2677.5683937072754 | Running corrects: 23\n",
      "Running loss: 2986.2708473205566 | Running corrects: 24\n",
      "Running loss: 3399.146945953369 | Running corrects: 25\n",
      "Running loss: 3708.5722999572754 | Running corrects: 27\n",
      "Running loss: 3941.5413856506348 | Running corrects: 29\n",
      "Running loss: 4226.416873931885 | Running corrects: 31\n",
      "Running loss: 4525.763614654541 | Running corrects: 33\n",
      "Running loss: 5058.737308502197 | Running corrects: 34\n",
      "Running loss: 5326.565464019775 | Running corrects: 36\n",
      "Running loss: 5326.565464019775 | Running corrects: 40\n",
      "train Loss: 66.5821 Acc: 0.5000 | Images trained on: 80\n",
      "Running loss: 213.76341247558594 | Running corrects: 2\n",
      "Running loss: 213.76341247558594 | Running corrects: 3\n",
      "val Loss: 42.7527 Acc: 0.6000 | Images trained on: 5\n",
      "Epoch 1/3\n",
      "----------\n",
      "Running loss: 164.57676696777344 | Running corrects: 2\n",
      "Running loss: 405.4566192626953 | Running corrects: 5\n",
      "Running loss: 655.4600677490234 | Running corrects: 7\n",
      "Running loss: 866.5862121582031 | Running corrects: 10\n",
      "Running loss: 1113.0673828125 | Running corrects: 11\n",
      "Running loss: 1202.928565979004 | Running corrects: 14\n",
      "Running loss: 1577.3942642211914 | Running corrects: 16\n",
      "Running loss: 1665.926773071289 | Running corrects: 18\n",
      "Running loss: 1965.6576690673828 | Running corrects: 20\n",
      "Running loss: 2020.8387908935547 | Running corrects: 23\n",
      "Running loss: 2336.952682495117 | Running corrects: 25\n",
      "Running loss: 2641.1722869873047 | Running corrects: 26\n",
      "Running loss: 2961.2947540283203 | Running corrects: 27\n",
      "Running loss: 3260.7670135498047 | Running corrects: 29\n",
      "Running loss: 3445.382568359375 | Running corrects: 31\n",
      "Running loss: 3773.187530517578 | Running corrects: 33\n",
      "Running loss: 4066.5244140625 | Running corrects: 35\n",
      "Running loss: 4643.158203125 | Running corrects: 36\n",
      "Running loss: 4883.13916015625 | Running corrects: 38\n",
      "Running loss: 4883.13916015625 | Running corrects: 42\n",
      "train Loss: 61.0392 Acc: 0.5250 | Images trained on: 80\n",
      "Running loss: 213.76341247558594 | Running corrects: 2\n",
      "Running loss: 213.76341247558594 | Running corrects: 3\n",
      "val Loss: 42.7527 Acc: 0.6000 | Images trained on: 5\n",
      "Epoch 2/3\n",
      "----------\n",
      "Running loss: 162.4774169921875 | Running corrects: 2\n",
      "Running loss: 320.87718200683594 | Running corrects: 5\n",
      "Running loss: 571.3983917236328 | Running corrects: 7\n",
      "Running loss: 695.7457580566406 | Running corrects: 10\n",
      "Running loss: 1089.4523315429688 | Running corrects: 11\n",
      "Running loss: 1125.4863548278809 | Running corrects: 14\n",
      "Running loss: 1486.8105430603027 | Running corrects: 16\n",
      "Running loss: 1569.151699066162 | Running corrects: 18\n",
      "Running loss: 1805.2320518493652 | Running corrects: 20\n",
      "Running loss: 1902.0435752868652 | Running corrects: 23\n",
      "Running loss: 2175.7758140563965 | Running corrects: 25\n",
      "Running loss: 2527.7059288024902 | Running corrects: 26\n",
      "Running loss: 2906.7371788024902 | Running corrects: 27\n",
      "Running loss: 3283.6960105895996 | Running corrects: 29\n",
      "Running loss: 3484.490749359131 | Running corrects: 31\n",
      "Running loss: 3766.8688316345215 | Running corrects: 33\n",
      "Running loss: 4052.3796348571777 | Running corrects: 35\n",
      "Running loss: 4778.077144622803 | Running corrects: 36\n",
      "Running loss: 5102.22384262085 | Running corrects: 38\n",
      "Running loss: 5102.22384262085 | Running corrects: 42\n",
      "train Loss: 63.7778 Acc: 0.5250 | Images trained on: 80\n",
      "Running loss: 213.76341247558594 | Running corrects: 2\n",
      "Running loss: 213.76341247558594 | Running corrects: 3\n",
      "val Loss: 42.7527 Acc: 0.6000 | Images trained on: 5\n",
      "Epoch 3/3\n",
      "----------\n",
      "Running loss: 168.6804962158203 | Running corrects: 2\n",
      "Running loss: 413.0513458251953 | Running corrects: 5\n",
      "Running loss: 642.0491180419922 | Running corrects: 7\n",
      "Running loss: 788.2035064697266 | Running corrects: 10\n",
      "Running loss: 1140.250015258789 | Running corrects: 11\n",
      "Running loss: 1180.410301208496 | Running corrects: 14\n",
      "Running loss: 1482.4119186401367 | Running corrects: 16\n",
      "Running loss: 1630.552635192871 | Running corrects: 18\n",
      "Running loss: 1957.7626266479492 | Running corrects: 20\n",
      "Running loss: 2063.47127532959 | Running corrects: 23\n",
      "Running loss: 2296.567771911621 | Running corrects: 25\n",
      "Running loss: 2640.9604721069336 | Running corrects: 26\n",
      "Running loss: 2965.7822189331055 | Running corrects: 27\n",
      "Running loss: 3261.5051803588867 | Running corrects: 29\n",
      "Running loss: 3509.197822570801 | Running corrects: 31\n",
      "Running loss: 3813.008918762207 | Running corrects: 33\n",
      "Running loss: 4105.541389465332 | Running corrects: 35\n",
      "Running loss: 4644.130012512207 | Running corrects: 36\n",
      "Running loss: 4938.645576477051 | Running corrects: 38\n",
      "Running loss: 4938.645576477051 | Running corrects: 42\n",
      "train Loss: 61.7331 Acc: 0.5250 | Images trained on: 80\n",
      "Running loss: 213.76341247558594 | Running corrects: 2\n",
      "Running loss: 213.76341247558594 | Running corrects: 3\n",
      "val Loss: 42.7527 Acc: 0.6000 | Images trained on: 5\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to train AlexNet on the small dataset\n",
    "# Don't expect accuracy to be very good because the dataset used is tiny\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.1)\n",
    "model_transfer = train_model(\n",
    "    DataUtilitys_smaller, \n",
    "    data_sizes_smaller,\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer_ft, \n",
    "    exp_lr_scheduler,\n",
    "    num_epochs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "latter-extra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.4136,  -0.2587,  -0.5166,   1.9010,  -0.0771,   0.7724,   1.6606],\n",
       "         [ 11.6954,  -2.1430,  -1.1246,   2.8636,   0.2056,  -0.8577,  -7.3382],\n",
       "         [ -5.1375,   7.9908, -30.9122,   2.8507,   7.4008,   0.3204,  34.5531],\n",
       "         [  0.6124,  -0.4629,  -1.1553,  -0.3769,   0.0432,  -0.2078,   0.5029]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 0.9510568380355835)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show an example of crossentropyloss.\n",
    "# Crossentropyloss expects \"raw, unnormalized scores\" for each class.\n",
    "sample = next(iter(DataUtilitys[\"train\"])) \n",
    "scores = model(sample[\"image\"])\n",
    "scores, criterion(scores, sample[\"label\"]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-component",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
