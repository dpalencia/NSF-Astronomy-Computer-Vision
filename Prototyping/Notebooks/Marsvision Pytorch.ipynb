{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attended-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing / Demonstrating the use of marsvision modules to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abandoned-strap",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dpale\\desktop\\projects\\marsvision\\marsvision\\vision\\ModelDefinitions.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_cfg)\n"
     ]
    }
   ],
   "source": [
    "from marsvision.pipeline import Model\n",
    "from marsvision.vision import ModelDefinitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "geographic-settlement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dpale/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "c:\\users\\dpale\\desktop\\projects\\marsvision\\marsvision\\pipeline\\Model.py:52: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  self.config = yaml.load(yaml_cfg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "alex_model = ModelDefinitions.alexnet()\n",
    "model = Model(alex_model, \"pytorch\", \n",
    "              dataset_root_directory=r\"C:\\Users\\dpale\\Desktop\\Projects\\marsvision\\test\\deep_mars_test_data\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "growing-atmosphere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/4\n",
      "----------\n",
      "Running loss: 158.29867553710938 | Running corrects: 0\n",
      "Running loss: 1486.8411560058594 | Running corrects: 0\n",
      "Running loss: 2251.4744567871094 | Running corrects: 1\n",
      "Running loss: 7639.553558349609 | Running corrects: 3\n",
      "Running loss: 7727.254203796387 | Running corrects: 3\n",
      "17\n",
      "train loss: 454.5444 Acc: 0.1765 | Images trained on: 17\n",
      "Running loss: 0.002633201191201806 | Running corrects: 1\n",
      "1\n",
      "val loss: 0.0026 Acc: 1.0000 | Images trained on: 1\n",
      "Epoch: 1/4\n",
      "----------\n",
      "Running loss: 613.86669921875 | Running corrects: 1\n",
      "Running loss: 613.8667199610572 | Running corrects: 5\n",
      "Running loss: 656.2306573389869 | Running corrects: 7\n",
      "Running loss: 798.6502435206276 | Running corrects: 9\n",
      "Running loss: 840.8688752650123 | Running corrects: 9\n",
      "17\n",
      "train loss: 49.4629 Acc: 0.5294 | Images trained on: 17\n",
      "Running loss: 0.0008672290714457631 | Running corrects: 1\n",
      "1\n",
      "val loss: 0.0009 Acc: 1.0000 | Images trained on: 1\n",
      "Epoch: 2/4\n",
      "----------\n",
      "Running loss: 223.9521026611328 | Running corrects: 1\n",
      "Running loss: 223.9521026611328 | Running corrects: 5\n",
      "Running loss: 358.7270965576172 | Running corrects: 6\n",
      "Running loss: 465.1383972167969 | Running corrects: 8\n",
      "Running loss: 492.05909156799316 | Running corrects: 8\n",
      "17\n",
      "train loss: 28.9447 Acc: 0.4706 | Images trained on: 17\n",
      "Running loss: 0.0008718741592019796 | Running corrects: 1\n",
      "1\n",
      "val loss: 0.0009 Acc: 1.0000 | Images trained on: 1\n",
      "Epoch: 3/4\n",
      "----------\n",
      "Running loss: 361.29229736328125 | Running corrects: 1\n",
      "Running loss: 361.336135417223 | Running corrects: 5\n",
      "Running loss: 437.98175767064095 | Running corrects: 8\n",
      "Running loss: 538.6940226852894 | Running corrects: 10\n",
      "Running loss: 561.8166022598743 | Running corrects: 10\n",
      "17\n",
      "train loss: 33.0480 Acc: 0.5882 | Images trained on: 17\n",
      "Running loss: 0.0008718741592019796 | Running corrects: 1\n",
      "1\n",
      "val loss: 0.0009 Acc: 1.0000 | Images trained on: 1\n",
      "Epoch: 4/4\n",
      "----------\n",
      "Running loss: 399.53936767578125 | Running corrects: 1\n",
      "Running loss: 399.53936767578125 | Running corrects: 5\n",
      "Running loss: 432.51851654052734 | Running corrects: 7\n",
      "Running loss: 519.3035125732422 | Running corrects: 9\n",
      "Running loss: 586.0243911743164 | Running corrects: 9\n",
      "17\n",
      "train loss: 34.4720 Acc: 0.5294 | Images trained on: 17\n",
      "Running loss: 0.0008718741592019796 | Running corrects: 1\n",
      "1\n",
      "val loss: 0.0009 Acc: 1.0000 | Images trained on: 1\n",
      "Best Epoch Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "yellow-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marsvision.utilities import DataUtility\n",
    "data_utility = DataUtility(r\"C:\\Users\\dpale\\Desktop\\Projects\\marsvision\\test\\deep_mars_test_data\\map-proj\")\n",
    "data_utility.data_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "taken-housing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 227, 227, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 4 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5e297749c48a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_utility\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\dpale\\desktop\\projects\\marsvision\\marsvision\\pipeline\\Model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, image_list)\u001b[0m\n\u001b[0;32m    112\u001b[0m             ])\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\marsvision\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\marsvision\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\marsvision\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_numpy_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pic should be 2/3 dimensional. Got {} dimensions.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 4 dimensions."
     ]
    }
   ],
   "source": [
    "output = model.predict(data_utility.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-league",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
