{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proud-editing",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset: 10.5281/zenodo.1048301\n",
    "from marsvision.utilities import DataUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regular-contractor",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_root = \"X:\\hirise-map-proj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "understood-terrace",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "class DeepMarsData(Dataset):\n",
    "    # Wrapper to work with deep mars dataset\n",
    "    def __init__(self, root_dir):\n",
    "        \n",
    "        # AlexNet expects images to be normalized this way.\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        # Get image labels\n",
    "        self.labels = {}\n",
    "        with open(os.path.join(root_dir, \"labels-map-proj.txt\")) as f:\n",
    "            for line in f:\n",
    "                items = line.split()\n",
    "                key, value = items[0], items[1]\n",
    "                self.labels[key] = int(value)\n",
    "                \n",
    "        # Get image filenames\n",
    "        self.image_dir = os.path.join(root_dir, \"map-proj\")\n",
    "        image_names = os.listdir(os.path.join(self.image_dir))\n",
    "        # Take set difference \n",
    "        # to ensure that only labelled images are included\n",
    "        self.image_names = list(set(image_names) & set(self.labels))\n",
    "                                  \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a sample as: {'image': image, 'label': label}\n",
    "        # Return an image with the dimensions 3 x W x H\n",
    "        # Because PyTorch models expect these dimensions as inputs.\n",
    "        # Transpose dimensions:\n",
    "        # (W, H, 3) --> (3, W, H)\n",
    "        img_name = self.image_names[idx]\n",
    "        img = Tensor(\n",
    "            cv2.imread(os.path.join(self.image_dir, img_name))\n",
    "        ).transpose(0, 2)\n",
    "        \n",
    "        # Apply normalize\n",
    "        img = self.normalize(img)\n",
    "    \n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"label\": self.labels[self.image_names[idx]]\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "insured-pacific",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Splt into train/validation/test sets\n",
    "# Train/Val/Test: 80/5/15\n",
    "dataset = DeepMarsData(dataset_root)\n",
    "\n",
    "# Define DataUtility for the whole dataset\n",
    "dataset_size = len(dataset)\n",
    "num_train_samples = int(dataset_size * .8)\n",
    "num_val_samples = int(dataset_size * .05)\n",
    "num_test_samples = dataset_size - num_train_samples - num_val_samples\n",
    "data_sizes = {\n",
    "    \"train\": num_train_samples,\n",
    "    \"val\": num_val_samples,\n",
    "    \"test\": num_test_samples\n",
    "}\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [num_train_samples, \n",
    "     num_val_samples, \n",
    "     num_test_samples]\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(train_dataset, batch_size = 4),\n",
    "    \"val\": torch.utils.data.DataLoader(train_dataset, batch_size = 4),\n",
    "    \"test\": torch.utils.data.DataLoader(train_dataset, batch_size = 4)\n",
    "}\n",
    "\n",
    "\n",
    "# Define a tiny datset to make quick tweaks to the model training code\n",
    "# Use a small subset of the dataset\n",
    "# So we can train it quickly.\n",
    "dataset_smaller, dataset_larger = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    [ 100, dataset_size - 100 ]\n",
    ")\n",
    "\n",
    "dataset_smaller_size = len(dataset_smaller)\n",
    "num_train_samples_smaller = int(dataset_smaller_size * .8)\n",
    "num_val_samples_smaller = int(dataset_smaller_size * .05)\n",
    "num_test_samples_smaller = dataset_smaller_size - num_train_samples_smaller - num_val_samples_smaller\n",
    "train_dataset_smaller, val_dataset_smaller, test_dataset_smaller = torch.utils.data.random_split(\n",
    "    dataset_smaller, \n",
    "    [num_train_samples_smaller, \n",
    "     num_val_samples_smaller, \n",
    "     num_test_samples_smaller]\n",
    ")\n",
    "\n",
    "data_sizes_smaller = {\n",
    "    \"train\": num_train_samples_smaller,\n",
    "    \"val\": num_val_samples_smaller,\n",
    "    \"test\": num_test_samples_smaller\n",
    "}\n",
    "\n",
    "dataloaders_smaller = {\n",
    "        \"train\": torch.utils.data.DataLoader(train_dataset_smaller, batch_size = 4),\n",
    "        \"val\": torch.utils.data.DataLoader(val_dataset_smaller, batch_size = 4),\n",
    "        \"test\": torch.utils.data.DataLoader(test_dataset_smaller, batch_size = 4)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ultimate-tomorrow",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dpale/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Alexnet and print its architecture.\n",
    "# Notice the last layer of the classifier.\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "guilty-queue",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(274.1620, grad_fn=<SelectBackward>),\n",
       " tensor([274.1620, 476.5874, 478.7771, 693.4528], grad_fn=<MaxBackward0>),\n",
       " tensor([111, 530, 530, 530]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Playing with the model/ Making predictions\n",
    "sample = next(iter(dataloaders[\"train\"])) \n",
    "output = model(sample[\"image\"]) # Output tensor of shape: (samples, # of classes)\n",
    "values, indices = torch.max(output, 1) # Get max values of confidence scores output by AlexNet.\n",
    "# Indices = classes\n",
    "# Use torch.max to return class label with highest confidence score.\n",
    "output[0][indices[0]], values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "directed-runner",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "# To change the number of output features, modify the classifier like so.\n",
    "# Classes correspond to indices.\n",
    "num_classes = 7\n",
    "model.classifier[6] = nn.Linear(4096,num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-dance",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a function to train the model\n",
    "def train_model(dataloaders, data_sizes, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            # Swap training/eval modes depending on phase\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                # Eval switches model's behavior\n",
    "                # Enable eval when we need to evaluate the model.\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate and train.\n",
    "            for sample in DataUtilitys[phase]:\n",
    "                inputs = Tensor(sample[\"image\"]).to(device)\n",
    "                labels = sample[\"label\"]\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass if in train phase\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                \n",
    "                print(\"Running loss: {} | Running corrects: {}\".format(\n",
    "                    running_loss, running_corrects))\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "                    \n",
    "            epoch_loss = running_loss / data_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / data_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} | Images trained on: {}'.format(\n",
    "                phase, epoch_loss, epoch_acc, data_sizes[phase]))\n",
    "            \n",
    "            # In the eval phase, get the accuracy for this epoch\n",
    "            # If the mode's current state is better than the best model seen so far,\n",
    "            # replace the best model weights\n",
    "            # with the previous best model weights on previous epochs\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-toronto",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to train AlexNet on the small dataset\n",
    "# Don't expect accuracy to be very good because the dataset used is tiny\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.1)\n",
    "model_transfer = train_model(\n",
    "    DataUtilitys_smaller, \n",
    "    data_sizes_smaller,\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer_ft, \n",
    "    exp_lr_scheduler,\n",
    "    num_epochs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-extra",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show an example of crossentropyloss.\n",
    "# Crossentropyloss expects \"raw, unnormalized scores\" for each class.\n",
    "sample = next(iter(DataUtilitys[\"train\"])) \n",
    "scores = model(sample[\"image\"])\n",
    "scores, criterion(scores, sample[\"label\"]).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}